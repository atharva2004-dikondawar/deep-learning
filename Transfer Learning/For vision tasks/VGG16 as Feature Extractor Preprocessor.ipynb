{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef507a00-d289-44ed-a9d6-501b10167cee",
   "metadata": {},
   "source": [
    "## Pre-Trained Model as Feature Extractor Preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803587a7-3ddd-4c52-a1c0-31f9c010cffd",
   "metadata": {},
   "source": [
    "The pre-trained model may be used as a standalone program to extract features from new photographs.\r\n",
    "\r\n",
    "Specifically, the extracted features of a photograph may be a vector of numbers that the model will use to describe the specific features in a photograph. These features can then be used as input in the development of a new model.\r\n",
    "\r\n",
    "The last few layers of the VGG16 model are fully connected layers prior to the output layer. These layers will provide a complex set of features to describe a given input image and may provide useful input when training a new model for image classification or related computer vision task.\r\n",
    "\r\n",
    "The image can be loaded and prepared for the model, as we did before in the previous example.\r\n",
    "\r\n",
    "We will load the model with the classifier output part of the model, but manually remove the final output layer. This means that the second last fully connected layer with 4,096 nodes will be the new output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c4cdeb-2b8c-4474-b2ed-5f99bebe1991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 270ms/step\n",
      "(1, 4096)\n"
     ]
    }
   ],
   "source": [
    "# example of using the vgg16 model as a feature extraction model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from pickle import dump\n",
    "\n",
    "# load an image from file\n",
    "image = load_img('dog.jpg', target_size=(224, 224))\n",
    "\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "\n",
    "# reshape data for the model\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "# prepare the image for the VGG model\n",
    "image = preprocess_input(image)\n",
    "\n",
    "# load model\n",
    "model = VGG16()\n",
    "\n",
    "# remove the output layer\n",
    "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "\n",
    "# get extracted features\n",
    "features = model.predict(image)\n",
    "print(features.shape)\n",
    "\n",
    "# save to file\n",
    "dump(features, open('dog.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cdacf8-93f1-4c37-867e-a0648a55e73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlenv)",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
